{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2c1ada",
   "metadata": {
    "id": "aa2c1ada"
   },
   "source": [
    "# Dreambooth\n",
    "### Notebook implementation by Joe Penna (@MysteryGuitarM on Twitter) - Improvements by David Bielejeski\n",
    "\n",
    "### Instructions\n",
    "- Sign up for RunPod here: https://runpod.io/?ref=n8yfwyum\n",
    "    - Note: That's my personal referral link. Please don't use it if we are mortal enemies.\n",
    "\n",
    "- Click *Deploy* on either `SECURE CLOUD` or `COMMUNITY CLOUD`\n",
    "\n",
    "- Follow the rest of the instructions in this video: https://www.youtube.com/watch?v=7m__xadX0z0#t=5m33.1s\n",
    "\n",
    "Latest information on:\n",
    "https://github.com/JoePenna/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b971cc0",
   "metadata": {
    "id": "7b971cc0"
   },
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2AsGA1xpNQnb",
   "metadata": {
    "id": "2AsGA1xpNQnb"
   },
   "outputs": [],
   "source": [
    "# If running on Vast.AI, copy the code in this cell into a new notebook. Run it, then launch the `dreambooth_runpod_joepenna.ipynb` notebook from the jupyter interface.\n",
    "!git clone https://github.com/JoePenna/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef27dc0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BUILD ENV\n",
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    " !pip install omegaconf\n",
    " !pip install einops\n",
    " !pip install pytorch-lightning==1.6.5\n",
    " !pip install test-tube\n",
    " !pip install transformers\n",
    " !pip install kornia\n",
    " !pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    " !pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    " !pip install setuptools==59.5.0\n",
    " !pip install pillow==9.0.1\n",
    " !pip install torchmetrics==0.6.0\n",
    " !pip install -e .\n",
    " !pip install protobuf==3.20.1\n",
    " !pip install gdown\n",
    " !pip install -qq diffusers[\"training\"]==0.3.0 transformers ftfy\n",
    " !pip install huggingface_hub\n",
    " !pip install captionizer==1.0.1\n",
    " !pip install ipywidgets\n",
    "\n",
    " clear_output()\n",
    " print(\"✅ Dependencies successfully installed\")\n",
    "\n",
    "except:\n",
    " print(\"❌ Error installing dependencies\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479fa8d",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Download the 1.5 model from hugging face\n",
    "You can also provide your own v1.* model for training by uploading it and renaming it to \"model.ckpt\".  It should be in the same directory as dreambooth_runpod_joepenna.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4739408",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the 1.5 sd model\n",
    "from IPython.display import clear_output\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "downloaded_model_path = hf_hub_download(\n",
    " repo_id=\"panopstor/EveryDream\",\n",
    " filename=\"sd_v1-5_vae.ckpt\"\n",
    ")\n",
    "\n",
    "# Move the sd_v1-5_vae.ckpt to the root of this directory as \"model.ckpt\"\n",
    "actual_locations_of_model_blob = !readlink -f {downloaded_model_path}\n",
    "!mv {actual_locations_of_model_blob[-1]} model.ckpt\n",
    "clear_output()\n",
    "print(\"✅ model.ckpt successfully downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dreambooth Training Environment Setup\n",
    "\n",
    "### Training Images\n",
    "WARNING: Be sure to upload an *even* amount of images, otherwise the training inexplicably stops at 1500 steps.\n",
    "\n",
    "*   2-3 full body\n",
    "*   3-5 upper body\n",
    "*   5-12 close-up on face\n",
    "\n",
    "The images should be:\n",
    "\n",
    "- as close as possible to the kind of images you're trying to make\n",
    "\n",
    "### Regularization Images\n",
    "We've created the following image sets\n",
    "\n",
    "`man_euler` - provided by Niko Pueringer (Corridor Digital) - euler @ 40 steps, CFG 7.5\n",
    "`man_unsplash` - pictures from various photographers\n",
    "`300_person_ddim`\n",
    "`person_ddim`\n",
    "`woman_ddim` - provided by David Bielejeski - ddim @ 50 steps, CFG 10.0\n",
    "`person_ddim` is recommended\n",
    "\n",
    "### Config\n",
    "\n",
    "These do what they say.  Eventually we'll integrate this into the config.\n",
    "\n",
    "* \"v1-finetune_unfrozen.yaml\"\n",
    "* \"v1-finetune_unfrozen_save_checkpoints_every_250_steps.yaml\"\n",
    "* \"v1-finetune_unfrozen_save_checkpoints_every_500_steps.yaml\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5713c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Setup your Dreambooth Training Environment\n",
    "%run juptyer-notebook-helpers/create-config-form-inputs.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e50df",
   "metadata": {
    "id": "ad4e50df"
   },
   "source": [
    "## Training\n",
    "\n",
    "If training a person or subject, keep an eye on your project's `logs/{folder}/images/train/samples_scaled_gs-00xxxx` generations.\n",
    "\n",
    "If training a style, keep an eye on your project's `logs/{folder}/images/train/samples_gs-00xxxx` generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
   "metadata": {
    "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# parse the config file \"joepenna-dreambooth-configs/active-config.json\"\n",
    "config_file_path = \"./joepenna-dreambooth-configs/active-config.json\";\n",
    "if not os.path.exists(config_file_path):\n",
    " print(f\"{config_file_path} not found.\", file=sys.stderr)\n",
    "else:\n",
    " config_file = open(config_file_path)\n",
    " config_parsed = json.load(config_file)\n",
    " print(\"✅ Loaded the config file, proceeding to training\")\n",
    " print()\n",
    " print(\"active-config.json\")\n",
    " print(\"------------------\")\n",
    " print(json.dumps(config_parsed, indent=4))\n",
    "\n",
    "\n",
    " flip_p_arg = 0.5\n",
    " if config_parsed.face_training:\n",
    "  flip_p_arg = 0.0\n",
    "\n",
    " reg_data_root = \"/workspace/Dreambooth-Stable-Diffusion/regularization_images/\" + config_parsed.dataset\n",
    "\n",
    " !rm -rf training_images/.ipynb_checkpoints\n",
    "\n",
    " !python \"main.py\" \\\n",
    "     --base \"configs/stable-diffusion/{config_parsed.config}\" \\\n",
    "     -t \\\n",
    "     --actual_resume \"model.ckpt\" \\\n",
    "     --reg_data_root \"{reg_data_root}\" \\\n",
    "     -n \"{config_parsed.project_name}\" \\\n",
    "     --gpus 0, \\\n",
    "     --data_root \"/workspace/Dreambooth-Stable-Diffusion/training_images\" \\\n",
    "     --max_training_steps {config_parsed.max_training_steps} \\\n",
    "     --class_word \"{config_parsed.class_word}\" \\\n",
    "     --token \"{config_parsed.token}\" \\\n",
    "     --no-test \\\n",
    "     --flip_p {flip_p_arg}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49d0bd",
   "metadata": {},
   "source": [
    "## Copy and name the checkpoint file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b05c16",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# parse the config file \"joepenna-dreambooth-configs/active-config.json\"\n",
    "config_file_path = \"./joepenna-dreambooth-configs/active-config.json\";\n",
    "if not os.path.exists(config_file_path):\n",
    " print(f\"{config_file_path} not found.\", file=sys.stderr)\n",
    "else:\n",
    " config_file = open(config_file_path)\n",
    " config_parsed = json.load(config_file)\n",
    " training_images = !find training_images/*\n",
    " date_string = !date +\"%Y-%m-%dT%H-%M-%S\"\n",
    "\n",
    " if config_parsed.config == \"v1-finetune_unfrozen.yaml\":\n",
    "  # Copy the checkpoint into our `trained_models` folder\n",
    "  directory_paths = !ls -d logs/*\n",
    "  last_checkpoint_file = directory_paths[-1] + \"/checkpoints/last.ckpt\"\n",
    "  file_name = date_string[-1] + \"_\" + \\\n",
    "              config_parsed.project_name + \"_\" + \\\n",
    "              str(len(training_images)) + \"_training_images_\" + \\\n",
    "              str(config_parsed.max_training_steps) + \"_max_training_steps_\" + \\\n",
    "              config_parsed.token + \"_token_\" + \\\n",
    "              config_parsed.class_word + \"_class_word.ckpt\"\n",
    "\n",
    "  file_name = file_name.replace(\" \", \"_\")\n",
    "\n",
    "  !mkdir -p trained_models\n",
    "  !mv \"{last_checkpoint_file}\" \"trained_models/{file_name}\"\n",
    "\n",
    "  print(\"Download your trained model from trained_models/\" + file_name + \" and use in your favorite Stable Diffusion repo!\")\n",
    " else:\n",
    "  directory_paths = !ls -d logs/*\n",
    "  checkpoints_directory = directory_paths[-1] + \"/checkpoints/trainstep_checkpoints\"\n",
    "  file_paths = !ls -d \"{checkpoints_directory}\"/*\n",
    "\n",
    "  for i, original_file_name in enumerate(file_paths):\n",
    "   # Remove the \"epoch=000000-step=0000\" text\n",
    "   steps = re.sub(checkpoints_directory + \"/epoch=\\d{6}-step=0*\", \"\", original_file_name)\n",
    "\n",
    "   # Remove the .ckpt\n",
    "   steps = steps.replace(\".ckpt\", \"\")\n",
    "\n",
    "   # Setup the filename\n",
    "   file_name = date_string[-1] + \"_\" + \\\n",
    "                   config_parsed.project_name + \"_\" + \\\n",
    "                   str(len(training_images)) + \"_training_images_\" + \\\n",
    "                   steps + \"_training_steps_\" + \\\n",
    "                   config_parsed.token + \"_token_\" + \\\n",
    "                   config_parsed.class_word + \"_class_word.ckpt\"\n",
    "\n",
    "   file_name = file_name.replace(\" \", \"_\")\n",
    "\n",
    "   # Make the directory and move the files into it.\n",
    "   !mkdir -p trained_models\n",
    "   !mv \"{original_file_name}\" \"trained_models/{file_name}\"\n",
    "\n",
    "  print(\"Download your trained models from the 'trained_models' folder and use in your favorite Stable Diffusion repo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f14dd",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optional - Upload to google drive\n",
    "* run the following commands in a new `terminal` in the `Dreambooth-Stable-Diffusion` directory\n",
    "* `chmod +x ./gdrive`\n",
    "* `./gdrive about`\n",
    "* `paste your token here after navigating to the link`\n",
    "* `./gdrive upload trained_models/{file_name.ckpt}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90ac5c",
   "metadata": {},
   "source": [
    "# Big Important Note!\n",
    "\n",
    "The way to use your token is `<token> <class>` ie `joepenna person` and not just `joepenna`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
